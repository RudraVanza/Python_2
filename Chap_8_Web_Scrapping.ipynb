{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Api Key :- 12fbda28d403fa5192821f78a33e9914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"12fbda28d403fa5192821f78a33e9914\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GEOCODING API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,requests\n",
    "city = input(\"Enter City Name :- \")\n",
    "url = \"http://api.openweathermap.org/geo/1.0/direct?q=\"+city+\"&appid=\"+api_key    #OR\n",
    "# url = f\"http://api.openweathermap.org/geo/1.0/direct?q={city},&appid={api_key}\"\n",
    "response = requests.get(url)\n",
    "cordinates = response.json() \n",
    "cordinates\n",
    "lat = cordinates[0]['lat']\n",
    "lon = cordinates[0]['lon']\n",
    "print(f\"let :- {lat} , lon :- {lon}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Current Wather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&units=metric&appid={api_key}\" #units=metric for fernhit to celcius\n",
    "response = requests.get(url)\n",
    "current_wather = response.json()\n",
    "print(json.dumps(current_wather , indent = 5))\n",
    "print(current_wather[\"weather\"][0][\"main\"])\n",
    "pressure = current_wather[\"main\"][ \"pressure\"]\n",
    "temp = current_wather[\"main\"][\"temp\"]\n",
    "humidity = current_wather[\"main\"][\"humidity\"]\n",
    "visibility = current_wather[ \"visibility\"]\n",
    "print(f\"pressure :- {pressure} \\n temp :- {temp} \\n humidity :- {humidity} \\n visibility :- {visibility} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Air Pollution Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://api.openweathermap.org/data/2.5/air_pollution?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "response = requests.get(url)\n",
    "airPolution = response.json()\n",
    "print(json.dumps(airPolution,indent=5))\n",
    "aqi = airPolution[\"list\"][0][\"main\"][\"aqi\"]\n",
    "des = {1 : \"Good\" , 2 : \"Fair\" , 3 : \"Moderate\" , 4 : \"Poor\" , 5 : \"Very Poor\"}\n",
    "qua = des[aqi]\n",
    "print(f\"Quality of air is {qua}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 5-day weather forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = f\"http://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "response = requests.get(url)\n",
    "fiveday = response.json()\n",
    "print(json.dumps(fiveday , indent=5))\n",
    "# Date & Time , Tamp , prassure , humidity , discription , \n",
    "d = {\"date_time\":[] , \"temp\":[] ,\"pressure\":[] , \"humidity\" : [] , \"descreption\" : []}\n",
    "for i in fiveday[\"list\"]:\n",
    "    d[\"date_time\"].append(i[\"dt_txt\"])\n",
    "    d[\"temp\"].append(i[\"main\"][\"temp\"])\n",
    "    d[\"pressure\"].append(i[\"main\"][\"pressure\"])\n",
    "    d[\"humidity\"].append(i[\"main\"][\"humidity\"])\n",
    "    d[\"descreption\"].append(i[\"weather\"][0][\"description\"])\n",
    "df = pd.DataFrame(d)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://subslikescript.com/movie/Titanic-120338\"\n",
    "result = requests.get(website)\n",
    "contant = result.text\n",
    "soup = BeautifulSoup(contant , \"html.parser\")\n",
    "print(soup.prettify())\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "title = soup.find(\"h1\").text\n",
    "print(title)\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "box = soup.find(\"article\" , class_= \"main-article\")\n",
    "box\n",
    "h1 = box.find(\"h1\").text\n",
    "print(h1)\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "transcript = box.find(\"div\" , class_ = \"full-script\").text.strip()\n",
    "print(transcript)\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "(transcript).encode(\"utf-8\")\n",
    "file = open(\"Titanic.txt\",\"w\")\n",
    "file.write(transcript)\n",
    "file.close()\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = \"https://indianexpress.com/?utm_source=google&utm_medium=cpc&utm_campaign=allaccess_brandkw&gad_source=1&gad_campaignid=22039452635&gbraid=0AAAAApGrV3KPgZUY1fNLn3eJZYQi63_nd&gclid=EAIaIQobChMIw-OW8ZbXjQMVV6JmAh3XhwwVEAAYASAAEgJrH_D_BwE\"\n",
    "result = requests.get(web)\n",
    "contant = result.text\n",
    "soup = BeautifulSoup(contant , \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "box = soup.find(\"div\" , class_=\"top-news\")\n",
    "headline = box.find_all(\"a\")\n",
    "print(headline)\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "for i in headline:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 9 Marks :- Where website will not give , they direct give us webpage , 30 most popular movies\n",
    "with open(\"30 Most Popular Movies Right Now_ What to Watch In Theaters and Streaming __ Rotten Tomatoes â€“ Movie and TV News.html\" , \"r\" , encoding=\"utf-8\") as file : \n",
    "    response =  file.read()\n",
    "# response\n",
    "soup = BeautifulSoup(response , \"html.parser\")\n",
    "box = soup.find_all(\"div\" , class_=\"row countdown-item\")\n",
    "title = []\n",
    "rating = []\n",
    "for i in box:\n",
    "    title.append(i.find(\"h2\").text[:-3])\n",
    "    rating.append(i.find(\"span\" , class_ = \"tMeterScore\").text)\n",
    "df = pd.DataFrame({\"title\" : title,\"rating\":rating})\n",
    "df.index = range(1, len(df) + 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name , mileage , dealer name , ratting , review count , price\n",
    "with open (\"Certified used Mercedes-Benz for sale.html\" , \"r\") as file:\n",
    "    response = file.read()\n",
    "soup = BeautifulSoup(response , \"html.parser\")\n",
    "box = soup.find_all(\"div\" , class_=\"vehicle-details\")\n",
    "box\n",
    "name = []\n",
    "mileage = []\n",
    "dealer_name = []\n",
    "rating = []\n",
    "review_count = []\n",
    "price = []\n",
    "for i in box:\n",
    "    name.append(i.find(\"h2\").text)\n",
    "    mileage.append(i.find(\"div\" , class_=\"mileage\").text)\n",
    "    dealer_name.append(i.find(\"strong\").text)\n",
    "    try:\n",
    "        rating.append(i.find(\"span\" , class_ = \"sds-rating__count\").text)\n",
    "    except :\n",
    "        rating.append(np.nan)\n",
    "    review_count.append(i.find(\"span\" , class_ = \"sds-rating__link sds-button-link\").text[1:-8])\n",
    "    price.append(i.find(\"span\" , class_=\"primary-price\").text)\n",
    "df = pd.DataFrame({\"Name\" : name , \"Mileage\" : mileage , \"Dealer Name\" : dealer_name , \"Rating\" : rating , \"Review Count\" : review_count , \"Price\" : price})\n",
    "df.index=range(1,len(df)+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Name :- Name of mobile , Rating , 1st price , Final price\n",
    "with open(\"flipkart.html\" , \"r\" ) as file:\n",
    "    response = file.read()\n",
    "soup = BeautifulSoup(response , \"html.parser\")\n",
    "box = soup.find_all(\"div\" , class_=\"_1AtVbE col-12-12\")\n",
    "name= []\n",
    "rating = []\n",
    "fiPrice = []\n",
    "fPrice = []\n",
    "for i in box:\n",
    "    name.append(i.find(\"div\" ,class_=\"_4rR01T\"))\n",
    "    rating.append(i.find(\"div\" , class_=\"_3LWZlK\"))\n",
    "    fiPrice.append(i.find(\"div\" ,class_=\"_30jeq3 _1_WHN1\" ))\n",
    "    try:\n",
    "        fPrice.append(i.find(\"div\", class_=\"_3I9_wc _27UcVY\").text)\n",
    "    except:\n",
    "        fPrice.append(np.nan)\n",
    "df = pd.DataFrame({\"Name\" : name , \"Rating\" : rating , \"First Price\" : fiPrice , \"Fianl Price\" : fPrice})\n",
    "df.index = range(1,len(df)+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the job , Name of Company , Job location , Expected Salary\n",
    "with open (\"indeed jobs.html\" , \"r\") as file:\n",
    "    response = file.read()\n",
    "soup = BeautifulSoup(response)\n",
    "box = soup.find_all(\"div\" , class_=\"job_seen_beacon\")\n",
    "name = []\n",
    "company = []\n",
    "location = []\n",
    "salary = []\n",
    "for i in box : \n",
    "    name.append(i.find(\"span\").text)\n",
    "    company.append(i.find(\"span\" , class_=\"css-1h7lukg eu4oa1w0\").text)\n",
    "    location.append(i.find(\"div\" , class_=\"css-1restlb eu4oa1w0\").text)\n",
    "    try:\n",
    "        salary.append(i.find(\"div\" , class_=\"css-18z4q2i eu4oa1w0\").text)\n",
    "    except:\n",
    "        salary.append(np.nan)\n",
    "df = pd.DataFrame({\"Name Of Job\":name , \"Name of Company\" : company , \"Job Location\" :location , \"Expected Salary\" : salary})\n",
    "df.index = range(1,len(df)+1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SENSEX Top Gainers, BSE Daily Top Gainers - Stock Market.html\" , \"r\") as file:\n",
    "    response =  file.read()\n",
    "soup = BeautifulSoup(response)\n",
    "box = soup.find(\"table\" , class_=\"common-table\")\n",
    "print(box)\n",
    "company = []\n",
    "current_Price= []\n",
    "prev_Close_Date = []\n",
    "change = []\n",
    "change_Per = []\n",
    "day_s_Low_High = []\n",
    "for i in box:\n",
    "    company.append(i.find(\"a\"))\n",
    "company"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
